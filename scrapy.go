package goscrapy

import (
	"net/http"
	"net/url"
	"sync"

	"github.com/PuerkitoBio/goquery"
)

// Request represents crawling request
type Request struct {
	Method       string      `json:"method,omitempty"`
	URL          string      `json:"url,omitempty"`
	Header       http.Header `json:"header,omitempty"`
	Query        url.Values  `json:"query,omitempty"`
	currentDepth int         // current request depth
	// TODO: add weight infoï¼Œlet goscrapy scheduler to sort all requests by weight
}

// Response represents crawling response
type Response struct {
	request  *Request
	Document *goquery.Document
	*http.Response
}

// Context represents the scraping and crawling context
type Context struct {
	resp    *Response
	aborted bool
}

// Response returns the http response
func (ctx *Context) Response() *Response {
	return ctx.resp
}

// Document returns HTML document
func (ctx *Context) Document() *goquery.Document {
	if ctx.resp == nil {
		return nil
	}
	return ctx.Response().Document
}

// Abort abort current context, the request will not been handled if current context was aborted.
// you could use it at your request middleware to make sure certain request will not be hanlded by
// downloader and spiders.
func (ctx *Context) Abort() {
	ctx.aborted = true
}

// IsAborted returns true if the current context was aborted.
func (ctx *Context) IsAborted() bool {
	return ctx.aborted
}

// Items items
type Items struct {
	sync.Map
	name string
}

// NewItems new items with specifed name, goscrapy pipeline will
// make it's decision whether to handle a items based on item's name.
func NewItems(name string) *Items {
	return &Items{
		Map:  sync.Map{},
		name: name,
	}
}

// Spider is an interface which defines how a certain site (or a group of sites) will be scraped,
// including how to perform the crawl (i.e. follow links) and how to extract structured data
// from their pages (i.e. scraping items). In other words, Spiders are the place where you define
// the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites).
// For spiders, the scraping cycle goes through something like this:
// 1. Using initial Requests generated by StartRequests to crawl the first urls.
// 2. Parsing the response (web page), then return items object (structured data) and request objects. Those requests
//    will be be added into scheduler by goscrapy engine and downloaded by downloader at some time in the future.
type Spider interface {
	Name() string
	StartRequests() []*Request
	URLMatcher() URLMatcher
	Parse(ctx *Context) (*Items, []*Request, error)
}

// Pipeline pipeline
type Pipeline interface {
	ItemList() []string // returns all items' name that this pipeline shoud handle
	Handle(ctx *Context, items *Items) error
}
